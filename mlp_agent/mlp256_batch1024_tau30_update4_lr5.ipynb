{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93dda018",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1cfcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython import display\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Use CPU# use cpu run\n",
    "print(device)\n",
    "import gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a63fe701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8e0e3",
   "metadata": {},
   "source": [
    "### 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d74676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array(state):\n",
    "    new_state = []\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])        \n",
    "    state = np.asarray(new_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8650bd",
   "metadata": {},
   "source": [
    "### 3. Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe07f4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: {'cumsumfert': 0.0, 'dap': 0, 'dtt': 0.0, 'ep': 0.0, 'grnwt': 0.0, 'istage': 7, 'nstres': 0.0, 'rtdep': 0.0, 'srad': 13.300000190734863, 'sw': array([0.086     , 0.086     , 0.086     , 0.086     , 0.086     ,\n",
      "       0.076     , 0.076     , 0.13      , 0.25799999]), 'swfac': 0.0, 'tmax': 22.200000762939453, 'topwt': 0.0, 'totir': 0.0, 'vstage': 0.0, 'wtdep': 0.0, 'xlai': 0.0}\n",
      "17 9\n",
      "\n",
      "Ram information received from DASSAT will has 25 dimensions.\n",
      "There are 25 possible actions at each step.\n",
      "Discrete? False\n"
     ]
    }
   ],
   "source": [
    "env_args = {\n",
    "    'run_dssat_location': '/opt/dssat_pdi/run_dssat',  # assuming (modified) DSSAT has been installed in /opt/dssat_pdi\n",
    "    'log_saving_path': './logs/dssat-pdi.log',  # if you want to save DSSAT outputs for inspection\n",
    "    # 'mode': 'irrigation',  # you can choose one of those 3 modes\n",
    "    # 'mode': 'fertilization',\n",
    "    'mode': 'all',\n",
    "    'seed': 123456,\n",
    "    'random_weather': False,  # if you want stochastic weather\n",
    "}\n",
    "env = gym.make('gym_dssat_pdi:GymDssatPdi-v0', **env_args)\n",
    "print('Observation:',env.observation,)\n",
    "print(len(env.observation),len(env.observation['sw']))\n",
    "ram_dimensions = 25\n",
    "nb_actions = 25\n",
    "print('\\nRam information received from DASSAT will has %d dimensions.' % ram_dimensions)\n",
    "print('There are %d possible actions at each step.' % nb_actions)\n",
    "print('Discrete?',type(gym.spaces)== gym.spaces.Discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4283d29",
   "metadata": {},
   "source": [
    "### 4. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6021276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Agent (Policy) Model.\"\"\"\n",
    "    # given a state of 35 dim, Qnetwork will return 200 values for each possible action  \n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1_units=256, fc2_units=256,fc3_units=256):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            why is it 256? randomly?\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        # set a nn with 1 layer\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "        # state_cross = torch.einsum('bi,bj->bij', state, state)\n",
    "        # state_combined = torch.cat((state.unsqueeze(2), state_cross), dim=2)\n",
    "        # state_flattened = torch.flatten(state_combined, start_dim=1)  \n",
    "        # state_cross = state * state\n",
    "        # state_combined = torch.cat((state, state_cross), dim=1)\n",
    "        x = F.relu(self.fc1(state))\n",
    "        y = F.relu(self.fc2(x))\n",
    "        z = F.relu(self.fc3(y))\n",
    "        #Applies the rectified linear unit function element-wise. max(0,x)\n",
    "        return self.fc4(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705878c",
   "metadata": {},
   "source": [
    "### 5. Define the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a8520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            how many samples stored in buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # store [s,a,r,s',done] for each sample in buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        # vstack is add rows together as a single matrix\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        #get all the states, actions, rewards, next_states, dones for all elements in this sample batch, each as a single matrix\n",
    "        #device here is cpu?\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72686c19",
   "metadata": {},
   "source": [
    "### 6. Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef9713bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 1024        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 30                # for update of target network parameters\n",
    "LR = 1e-5               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8e9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR, weight_decay=0.001)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        # add current (s,r,a,s') and sample a batch and learn if update_every\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step += 1\n",
    "        if self.t_step%UPDATE_EVERY == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        self.qnetwork_local.train()\n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        # return the action index with the largest value\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            #return one action randomly with possibility eps\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "        # detach for diemnsion correctness\n",
    "        # GET largest value of self.qnetwork_target(next_states)\n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # 0 if done\n",
    "\n",
    "        # Get expected Q values from local model\n",
    "        # print('out',self.qnetwork_local(states).shape)\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions)\n",
    "        #Why need to gather? actions are indexs?\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # after this, the parameter of local network will change based on gradient descent\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        # stabilize traning to keep grad between (-1,1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #Update target network weights every TAU learning steps (so every TAU*UPDATE_EVERY t_step)\n",
    "        if self.t_step%(TAU*UPDATE_EVERY)==0:\n",
    "            self.update_target_net(self.qnetwork_local, self.qnetwork_target)                     \n",
    "\n",
    "    def update_target_net(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "        \"\"\"\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "    def save(self,name):\n",
    "        return\n",
    "        # torch.save(self.qnetwork_local.state_dict(),'./model/model'+name+'.pth')\n",
    "        # print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "542a5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, n_action, w_action, next_state, done, k1, k2, k3):\n",
    "    if done:\n",
    "        reward = k1*state[4] - k2*n_action - k3*w_action\n",
    "        return reward\n",
    "    else:\n",
    "        reward = -k2*n_action - k3*w_action \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171c08a",
   "metadata": {},
   "source": [
    "### DQN for nitrogen management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8bd30bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(state_size=ram_dimensions, action_size=nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7bd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=500, eps_start=1.0, eps_end=0, eps_decay=0.994, solved=8000, optimize=True, exp=0):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    start = time.time()                # Start time\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    list_eps = []\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    n_amount = 0\n",
    "    w_amount = 0\n",
    "    n_amount_list = []\n",
    "    n_amount_window = deque(maxlen=100)\n",
    "    w_amount_list = []\n",
    "    w_amount_window = deque(maxlen=100)\n",
    "    leaching_sum_list = []\n",
    "    leaching_sum_window = deque(maxlen=100)\n",
    "    yield_list = []\n",
    "    yield_window = deque(maxlen=100)\n",
    "    y = 0\n",
    "    # Update training policy here by updating the weights below\n",
    "    k1 = 0.158\n",
    "    k2 = 0.79\n",
    "    k3 = 1.1\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        # Reset env and score at the beginning of episode\n",
    "        state = env.reset() # reset the environment and get current state\n",
    "        state = dict2array(state)\n",
    "        score = 0 # initialize the score\n",
    "        n_amount = 0\n",
    "        w_amount = 0\n",
    "        yield_amount = 0\n",
    "        leaching_sum = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action1 = agent.act(state, eps) if optimize else 12\n",
    "            action = {\n",
    "                'anfer': (action1 % 5) * 40,  # if mode == fertilization or mode == all; nitrogen to fertilize in kg/ha\n",
    "                'amir': int(action1 / 5) * 6,  # if mode == irrigation or mode == all; water to irrigate in L/ha\n",
    "            }\n",
    "            next_state, reward, done, _ = env.step(action)  # send the action to the environment\n",
    "            \n",
    "            if done:\n",
    "                y = state[4]\n",
    "                next_state = state\n",
    "                reward = get_reward(state, action['anfer'], action['amir'], next_state, done, k1, k2, k3)\n",
    "                agent.step(state, action1, reward, next_state, done)\n",
    "                score += reward\n",
    "                n_amount_list.append(n_amount)\n",
    "                w_amount_list.append(w_amount)\n",
    "                n_amount_window.append(n_amount)\n",
    "                w_amount_window.append(w_amount)\n",
    "                yield_list.append(y)\n",
    "                yield_window.append(y)\n",
    "                yield_amount = y\n",
    "                break\n",
    "            \n",
    "            n_amount += action['anfer']\n",
    "            w_amount += action['amir']\n",
    "            next_state = dict2array(next_state)\n",
    "            reward = get_reward(state, action['anfer'], action['amir'], next_state, done, k1, k2, k3)\n",
    "            agent.step(state, action1, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        \n",
    "        if score > 1460:\n",
    "            agent.save(str(i_episode))\n",
    "        \n",
    "        eps = max(eps_end, eps_decay * eps)  # decrease epsilon\n",
    "        list_eps.append(eps)\n",
    "        \n",
    "        if i_episode % 1 == 0:\n",
    "            print('\\rEpisode {}/{} \\t Score: {:.2f}'.format(i_episode, n_episodes, score))\n",
    "            print('Epsilon: {}'.format(eps))\n",
    "            print('Nitrogen amount is', np.mean(n_amount))\n",
    "            print('Irrigation amount is', w_amount)\n",
    "            print('Yield amount is', yield_amount)\n",
    "            \n",
    "        if np.mean(scores_window) > solved:\n",
    "            print('Game Solved after {} episodes'.format(i_episode))\n",
    "            break\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    print(\"Time Elapse: {:.2f} seconds\")\n",
    "    \n",
    "    return scores, list_eps, n_amount_list, w_amount_list, yield_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4c0b52-a50b-4428-82d0-2321755dd107",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, list_eps, n_amount_list, w_amount_list, yield_list = dqn(n_episodes=3000,exp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69485f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for subplots\n",
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "# Plot each dataset on its own subplot\n",
    "axs[0, 0].plot(scores)\n",
    "axs[0, 0].set_title('Scores')\n",
    "\n",
    "axs[0, 1].plot(list_eps)\n",
    "axs[0, 1].set_title('List Eps')\n",
    "\n",
    "axs[1, 0].plot(n_amount_list)\n",
    "axs[1, 0].set_title('N Amount List')\n",
    "\n",
    "axs[1, 1].plot(w_amount_list)\n",
    "axs[1, 1].set_title('W Amount List')\n",
    "\n",
    "axs[2, 0].plot(yield_list)\n",
    "axs[2, 0].set_title('yield Amount List')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig('mlp256_batch512_tau8_update16_lr5_aug_train1.pdf')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ebb1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Scores': scores,\n",
    "    'List_eps': list_eps,\n",
    "    'N_amount_list': n_amount_list,\n",
    "    'W_amount_list': w_amount_list,\n",
    "    'Yield_list': yield_list\n",
    "})\n",
    "\n",
    "# Save DataFrame and write to Excel\n",
    "df.to_excel('mlp256_batch512_tau8_update16_lr5_aug_train1.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8875d0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row with max value in column 'c' among rows where 'a' < 300 and 'b' < 300:\n",
      "Scores           1.372704e+03\n",
      "List_eps         2.382857e-07\n",
      "N_amount_list    2.400000e+02\n",
      "W_amount_list    1.620000e+02\n",
      "Yield_list       1.101585e+04\n",
      "Name: 2533, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your data from the Excel file\n",
    "data = pd.read_excel('mlp256_batch512_tau8_update16_lr5_aug_train1.xlsx')\n",
    "\n",
    "# Apply filters to select rows where column 'a' and column 'b' are both less than 300\n",
    "filtered_data = data[(data['N_amount_list'] < 300) & (data['W_amount_list'] < 300)]\n",
    "\n",
    "# Find the row with the maximum value in column 'c' among the filtered rows\n",
    "max_row = filtered_data.loc[filtered_data['Yield_list'].idxmax()]\n",
    "\n",
    "print(\"Row with max value in column 'c' among rows where 'a' < 300 and 'b' < 300:\")\n",
    "print(max_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3c37a-9ea2-4aba-accd-4296a3978b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gym DSSAT PDI",
   "language": "python",
   "name": "gym_dssat_pdi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
