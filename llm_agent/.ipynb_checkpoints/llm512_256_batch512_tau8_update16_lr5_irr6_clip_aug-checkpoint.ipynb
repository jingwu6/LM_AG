{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93dda018",
   "metadata": {},
   "source": [
    "### 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde9ecfe-612f-40b3-97de-4cc8e263d235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from IPython import display\n",
    "from collections import namedtuple, deque\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Use GPU\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Use CPU# use cpu run\n",
    "print(device)\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63fe701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a8e0e3",
   "metadata": {},
   "source": [
    "### 2. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d74676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict2array(state):\n",
    "    new_state = []\n",
    "    for key  in state.keys():\n",
    "        if key != 'sw':\n",
    "            new_state.append(state[key])\n",
    "        else:\n",
    "            new_state += list(state['sw'])        \n",
    "    state = np.asarray(new_state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8650bd",
   "metadata": {},
   "source": [
    "### 3. Initialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe07f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_args = {\n",
    "    'run_dssat_location': '/opt/dssat_pdi/run_dssat',  # assuming (modified) DSSAT has been installed in /opt/dssat_pdi\n",
    "    'log_saving_path': './logs/dssat-pdi.log',  # if you want to save DSSAT outputs for inspection\n",
    "    # 'mode': 'irrigation',  # you can choose one of those 3 modes\n",
    "    # 'mode': 'fertilization',\n",
    "    'mode': 'all',\n",
    "    'seed': 123456,\n",
    "    'random_weather': True,  # if you want stochastic weather\n",
    "}\n",
    "env = gym.make('gym_dssat_pdi:GymDssatPdi-v0', **env_args)\n",
    "print('Observation:',env.observation,)\n",
    "print(len(env.observation),len(env.observation['sw']))\n",
    "ram_dimensions = 25\n",
    "#ram_dimnetion = 9 if partial\n",
    "nb_actions = 25\n",
    "token_size = 27\n",
    "print('\\nRam information received from DASSAT will has %d dimensions.' % ram_dimensions)\n",
    "print('There are %d possible actions at each step.' % nb_actions)\n",
    "print('Discrete?',type(gym.spaces)== gym.spaces.Discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4283d29",
   "metadata": {},
   "source": [
    "### 4. Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a7ff9d-f7c0-4c5c-9a75-09088f9736ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import DistilBertModel, DistilBertTokenizer, BertTokenizerFast\n",
    "from torch import nn\n",
    "\n",
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, use_fast=True)\n",
    "distilbert_model = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "total_params = sum(p.numel() for p in distilbert_model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, fc1_units=512, fc2_units=256, fc3_units=256):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.distilbert = distilbert_model\n",
    "        self.fc1 = nn.Linear(distilbert_model.config.hidden_size * state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, action_size)\n",
    "        # self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        # self.fc4 = nn.Linear(fc3_units, action_size)\n",
    "        # 冻结 DistilBERT 的参数\n",
    "        # for param in self.distilbert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # with torch.no_grad():  # 禁用梯度计算以避免冻结的参数被更新\n",
    "        distilbert_out = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Extract the last hidden state or pooled output\n",
    "        last_hidden_state = distilbert_out.last_hidden_state\n",
    "        reshaped_hidden_states = last_hidden_state.view(last_hidden_state.shape[0], -1)\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        fc1_out = F.relu(self.fc1(reshaped_hidden_states))\n",
    "        fc2_out = F.relu(self.fc2(fc1_out))\n",
    "        # fc3_out = F.relu(self.fc3(fc2_out))\n",
    "        return self.fc3(fc2_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f36b15-12dd-4176-92e6-9fc42df5edb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f705878c",
   "metadata": {},
   "source": [
    "### 5. Define the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a8520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            action_size (int): dimension of each action\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            how many samples stored in buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        # store [s,a,r,s',done] for each sample in buffer\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states = np.vstack([e.state for e in experiences if e is not None])\n",
    "        # vstack is add rows together as a single matrix\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        #get all the states, actions, rewards, next_states, dones for all elements in this sample batch, each as a single matrix\n",
    "        #device here is cpu?\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72686c19",
   "metadata": {},
   "source": [
    "### 6. Define the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9713bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 512        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 8                 # for update of target network parameters\n",
    "LR = 1e-5               # learning rate \n",
    "betas=(0.9, 0.999)\n",
    "weight_decay=0.001\n",
    "UPDATE_EVERY = 16       # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbcfd1c-fc78-4c5f-8440-215c060e3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def array2str(state):\n",
    "    state_str = \"\"\n",
    "    for i, num in enumerate(state):\n",
    "        if i == 0:\n",
    "            state_str += str(round(num / 40)) + \" \"\n",
    "        elif i == 4:\n",
    "            state_str += str(round(num / 100)) + \" \"\n",
    "        elif i == 7:\n",
    "            state_str += str(round(num / 10)) + \" \"\n",
    "        elif i == 20:\n",
    "            state_str += str(round(num / 100)) + \" \" # 250\n",
    "        elif i == 21:\n",
    "            state_str += str(round(num / 6)) + \" \" # 12\n",
    "        elif i == 23:\n",
    "            state_str += str(round(num)) + \" \" # 10\n",
    "        elif i >= 9 and i <= 17:\n",
    "            state_str += str(round(num * 1000)) + \" \"\n",
    "        elif i == 18:\n",
    "            state_str += str(round(num * 100)) + \" \"\n",
    "        else:\n",
    "            state_str += str(round(num)) + \" \"\n",
    "        \n",
    "    return state_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(token_size, action_size).to(device)\n",
    "        self.qnetwork_target = QNetwork(token_size, action_size).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR, betas=betas)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        # Save experience in replay memory\n",
    "        # add current (s,r,a,s') and sample a batch and learn if update_every\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        if done:\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step += 1\n",
    "        if self.t_step%UPDATE_EVERY == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > BATCH_SIZE:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): current state\n",
    "            eps (float): epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"        \n",
    "        state_str = array2str(state)   \n",
    "        token = tokenizer(state_str, add_special_tokens=True, max_length=token_size, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        input_ids = token[\"input_ids\"].to(device)\n",
    "        attention_mask = token[\"attention_mask\"].to(device)\n",
    "        \n",
    "        self.qnetwork_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(input_ids, attention_mask)\n",
    "            \n",
    "        self.qnetwork_local.train()\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        state_str_list = []\n",
    "        for state in states:\n",
    "            state_str = array2str(state)\n",
    "            state_str_list.append(state_str)\n",
    "\n",
    "        token = tokenizer(state_str_list, add_special_tokens=True, max_length=token_size, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        input_ids_batch = token['input_ids'].to(device)\n",
    "        attention_mask_batch = token['attention_mask'].to(device)\n",
    "\n",
    "        next_state_str_list = []\n",
    "        for next_state in next_states:\n",
    "            next_state_str = array2str(next_state)\n",
    "            next_state_str_list.append(next_state_str)\n",
    "\n",
    "        next_token = tokenizer(next_state_str_list, add_special_tokens=True, max_length=token_size, truncation=True, padding='max_length', return_tensors='pt')\n",
    "        next_input_ids_batch = next_token[\"input_ids\"].to(device)\n",
    "        next_attention_mask_batch = next_token[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_input_ids_batch, next_attention_mask_batch).detach().max(1)[0].unsqueeze(1)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        Q_expected = self.qnetwork_local(input_ids_batch, attention_mask_batch).gather(1, actions)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # after this, the parameter of local network will change based on gradient descent\n",
    "        for param in self.qnetwork_local.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "        # stabilize traning to keep grad between (-1,1)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        #Update target network weights every TAU learning steps (so every TAU*UPDATE_EVERY t_step)\n",
    "        if self.t_step%(TAU*UPDATE_EVERY)==0:\n",
    "            self.update_target_net(self.qnetwork_local, self.qnetwork_target)                     \n",
    "\n",
    "    def update_target_net(self, local_model, target_model):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model): weights will be copied from\n",
    "            target_model (PyTorch model): weights will be copied to\n",
    "        \"\"\"\n",
    "        target_model.load_state_dict(local_model.state_dict())\n",
    "    def save(self,name):\n",
    "        torch.save(self.qnetwork_local.state_dict(),'./checkpoint/model'+name+'.pth')\n",
    "        print('saved')\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a5b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state, n_action, w_action, next_state, done, k1, k2, k3, k4):\n",
    "    if done:\n",
    "        reward = k1*state[4] - k2*n_action - k3*w_action\n",
    "        return reward\n",
    "    else:\n",
    "        reward = -k2*n_action - k3*w_action \n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171c08a",
   "metadata": {},
   "source": [
    "### DQN for nitrogen management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd30bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(state_size=ram_dimensions, action_size=nb_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7bd637",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "def dqn(n_episodes=2000, max_t=500, eps_start=1.0, eps_end=0, eps_decay=0.994, solved=8000, optimize=True, exp=0):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    start = time.time()                # Start time\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    list_eps = []\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    n_amount = 0\n",
    "    w_amount = 0\n",
    "    yield_amount = 0\n",
    "    n_amount_list = []\n",
    "    n_amount_window = deque(maxlen=100)\n",
    "    w_amount_list = []\n",
    "    w_amount_window = deque(maxlen=100)\n",
    "    leaching_sum_list = []\n",
    "    leaching_sum_window = deque(maxlen=100)\n",
    "    yield_list = []\n",
    "    yield_window = deque(maxlen=100)\n",
    "    y = 0\n",
    "    # Update training policy here by updating the weights below\n",
    "    k1 = 0.158\n",
    "    k2 = 0.79\n",
    "    k3 = 1.1\n",
    "    k4 = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "\n",
    "        time1 = time.time()\n",
    "        \n",
    "        # Reset env and score at the beginning of episode\n",
    "        state = env.reset() # reset the environment and get current state\n",
    "        state = dict2array(state)\n",
    "        score = 0 # initialize the score\n",
    "        n_amount = 0\n",
    "        w_amount = 0\n",
    "        leaching_sum = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            action1 = agent.act(state, eps) if optimize else 12\n",
    "            action = {\n",
    "                'anfer': (action1 % 5) * 40,  # if mode == fertilization or mode == all; nitrogen to fertilize in kg/ha\n",
    "                'amir': int(action1 / 5) * 6,  # if mode == irrigation or mode == all; water to irrigate in L/ha\n",
    "            }\n",
    "            if state[0] >= 10000:\n",
    "                action['anfer'] = 0\n",
    "            if state[21] >= 1600:\n",
    "                action['amir'] = 0\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)  # send the action to the environment\n",
    "            \n",
    "            if done:\n",
    "                y = state[4]\n",
    "                yield_amount = y\n",
    "                next_state = state\n",
    "                reward = get_reward(state, action['anfer'], action['amir'], next_state, done, k1, k2, k3, k4)\n",
    "                agent.step(state, action1, reward, next_state, done)\n",
    "                score += reward\n",
    "                n_amount_list.append(n_amount)\n",
    "                w_amount_list.append(w_amount)\n",
    "                n_amount_window.append(n_amount)\n",
    "                w_amount_window.append(w_amount)\n",
    "                leaching_sum_window.append(leaching_sum)\n",
    "                yield_list.append(y)\n",
    "                yield_window.append(y)\n",
    "                break\n",
    "            \n",
    "            n_amount += action['anfer']\n",
    "            w_amount += action['amir']\n",
    "            next_state = dict2array(next_state)\n",
    "            reward = get_reward(state, action['anfer'], action['amir'], next_state, done, k1, k2, k3, k4)\n",
    "            agent.step(state, action1, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "        \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        \n",
    "        if score > 1400 and yield_amount > 11000:\n",
    "            agent.save(str(i_episode))\n",
    "        \n",
    "        eps = max(eps_end, eps_decay * eps)  # decrease epsilon\n",
    "        list_eps.append(eps)\n",
    "        \n",
    "        if i_episode % 1 == 0:\n",
    "            print('\\rEpisode {}/{} \\t Score: {:.2f}'.format(i_episode, n_episodes, score))\n",
    "            print('Epsilon: {}'.format(eps))\n",
    "            print('Nitrogen amount is', n_amount)\n",
    "            print('Irrigation amount is', w_amount)\n",
    "            print('Yield amount is', yield_amount)\n",
    "            \n",
    "        if np.mean(scores_window) > solved:\n",
    "            print('Game Solved after {} episodes'.format(i_episode))\n",
    "            break\n",
    "\n",
    "        time2 = time.time()\n",
    "        print(\"one epoch time:\", time2 - time1)\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    print(\"Time Elapse: {:.2f} seconds\")\n",
    "    \n",
    "    return scores, list_eps, n_amount_list, w_amount_list, yield_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c01a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores, list_eps, n_amount_list, w_amount_list, yield_list = dqn(n_episodes=3000,exp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69485f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure and axes for subplots\n",
    "fig, axs = plt.subplots(3, 2)\n",
    "\n",
    "# Plot each dataset on its own subplot\n",
    "axs[0, 0].plot(scores)\n",
    "axs[0, 0].set_title('Scores')\n",
    "\n",
    "axs[0, 1].plot(list_eps)\n",
    "axs[0, 1].set_title('List Eps')\n",
    "\n",
    "axs[1, 0].plot(n_amount_list)\n",
    "axs[1, 0].set_title('N Amount List')\n",
    "\n",
    "axs[1, 1].plot(w_amount_list)\n",
    "axs[1, 1].set_title('W Amount List')\n",
    "\n",
    "axs[2, 0].plot(yield_list)\n",
    "axs[2, 0].set_title('yield Amount List')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as a PDF file\n",
    "plt.savefig('combined_plots_distilbert.pdf')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4bfde4-0cec-43c2-9e17-c6f957b3fcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Length of scores:\", len(scores))\n",
    "print(\"Length of list_eps:\", len(list_eps))\n",
    "print(\"Length of n_amount_list:\", len(n_amount_list))\n",
    "print(\"Length of w_amount_list:\", len(w_amount_list))\n",
    "print(\"Length of yield_list:\", len(yield_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b896db6a-d8b8-43bf-8007-d0e570ee828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Scores': scores,\n",
    "    'List_eps': list_eps,\n",
    "    'N_amount_list': n_amount_list,\n",
    "    'W_amount_list': w_amount_list,\n",
    "    'Yield_list': yield_list\n",
    "})\n",
    "\n",
    "# save DataFrame to Excel file\n",
    "df.to_excel('data_distilbert.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Gym DSSAT PDI",
   "language": "python",
   "name": "gym_dssat_pdi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
